{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ConsoleDot service in Go This projects aims to cover all basic concepts needed for development of service for console.redhat.com. It aims to be a basic API serving service with a database. Repository structure All packages live under /internal to denote we do not intend to share these with other apps. All binaries have a directory under /cmd these are the app entrypoints. Make targets are under /mk directory. Config is under /config directory.","title":"Home"},{"location":"#consoledot-service-in-go","text":"This projects aims to cover all basic concepts needed for development of service for console.redhat.com. It aims to be a basic API serving service with a database.","title":"ConsoleDot service in Go"},{"location":"#repository-structure","text":"All packages live under /internal to denote we do not intend to share these with other apps. All binaries have a directory under /cmd these are the app entrypoints. Make targets are under /mk directory. Config is under /config directory.","title":"Repository structure"},{"location":"concepts/01-documentation/","text":"Documentation for your project This is a bit meta you say, sure it is! :) We believe starting with documentation is important, so you don't need to catch up later. Architecture Start with simple architecture of your project. You've sure brainstormed it before starting, just put in the docs folder what you have! Diagrams can be plain png s of course, but if you want to start with something better, do it with PlantUML. It helps you keep it up to date, because you keep the source code of the diagram with the code. Architecture Decision Records These Markdown documents are meant to document major decisions you've taken during your project development. Changing decisions is easy with a follow-up ADR, but you get an awesome benefit of the track record for the decisions. That's very helpful for every new developer, like you after few months working on something else ;) See more for example at adr.GitHub.io","title":"Documentation"},{"location":"concepts/01-documentation/#documentation-for-your-project","text":"This is a bit meta you say, sure it is! :) We believe starting with documentation is important, so you don't need to catch up later.","title":"Documentation for your project"},{"location":"concepts/01-documentation/#architecture","text":"Start with simple architecture of your project. You've sure brainstormed it before starting, just put in the docs folder what you have! Diagrams can be plain png s of course, but if you want to start with something better, do it with PlantUML. It helps you keep it up to date, because you keep the source code of the diagram with the code.","title":"Architecture"},{"location":"concepts/01-documentation/#architecture-decision-records","text":"These Markdown documents are meant to document major decisions you've taken during your project development. Changing decisions is easy with a follow-up ADR, but you get an awesome benefit of the track record for the decisions. That's very helpful for every new developer, like you after few months working on something else ;) See more for example at adr.GitHub.io","title":"Architecture Decision Records"},{"location":"concepts/02-makefile/","text":"Makefile Golan has a tooling around many commands. We believe it is helpful to customize these commands for actual project needs. We help document all the current targets by providing the make help we encourage you to continue this effort. It makes it very easy for new developers to see what common actions are needed for working on the project. We drafted the Makefile targets split under mk directory, so it's easier to keep targets grouped by their purpose.","title":"Makefile"},{"location":"concepts/02-makefile/#makefile","text":"Golan has a tooling around many commands. We believe it is helpful to customize these commands for actual project needs. We help document all the current targets by providing the make help we encourage you to continue this effort. It makes it very easy for new developers to see what common actions are needed for working on the project. We drafted the Makefile targets split under mk directory, so it's easier to keep targets grouped by their purpose.","title":"Makefile"},{"location":"concepts/03-building/","text":"Building your project Exciting! We are getting somewhere :) You can run your app by: make run And you can build a container for it locally by: make build-podman Following information is all about what happens when you run these commands :) Go dependencies We have three make targets for working with dependencies: make download-deps installs dependencies locally (aliased make prep ) make update-deps updates dependencies to the newest versions make tidy-deps cleans up dependencies locally The binary The main binary is called api and it serves as api application http server for our service. It's entry point is cmd/api/main.go Containerization Your app will run in the production environment in Container in OpenShift. So lets package our app in a Container :) You can build the container with Podman by running make build-podman . There are two phases in the build. It is captured in Containerfile . First is the build phase. We use the official Red Hat go-toolset build container to build our projects. We have manually set the go version of the container, when bumping the go version, it needs to be bumped here. The build itself is done by copying all project files in /build directory in the build container. Followed by running make prep build strip which runs phases: Install dependencies Building binaries Stripping binaries of debug information (to keep them smaller). FROM registry.access.redhat.com/ubi8/go-toolset:1:18 as build USER 0 RUN mkdir /build WORKDIR /build COPY . . RUN make prep build strip Second phase produces the final container image It just copies the binaries from the build container. FROM registry.access.redhat.com/ubi8/ubi-minimal:latest COPY --from=build /build/api /api USER 1001 CMD [\"/api\"] We are done :)","title":"Building"},{"location":"concepts/03-building/#building-your-project","text":"Exciting! We are getting somewhere :) You can run your app by: make run And you can build a container for it locally by: make build-podman Following information is all about what happens when you run these commands :)","title":"Building your project"},{"location":"concepts/03-building/#go-dependencies","text":"We have three make targets for working with dependencies: make download-deps installs dependencies locally (aliased make prep ) make update-deps updates dependencies to the newest versions make tidy-deps cleans up dependencies locally","title":"Go dependencies"},{"location":"concepts/03-building/#the-binary","text":"The main binary is called api and it serves as api application http server for our service. It's entry point is cmd/api/main.go","title":"The binary"},{"location":"concepts/03-building/#containerization","text":"Your app will run in the production environment in Container in OpenShift. So lets package our app in a Container :) You can build the container with Podman by running make build-podman . There are two phases in the build. It is captured in Containerfile .","title":"Containerization"},{"location":"concepts/03-building/#first-is-the-build-phase","text":"We use the official Red Hat go-toolset build container to build our projects. We have manually set the go version of the container, when bumping the go version, it needs to be bumped here. The build itself is done by copying all project files in /build directory in the build container. Followed by running make prep build strip which runs phases: Install dependencies Building binaries Stripping binaries of debug information (to keep them smaller). FROM registry.access.redhat.com/ubi8/go-toolset:1:18 as build USER 0 RUN mkdir /build WORKDIR /build COPY . . RUN make prep build strip","title":"First is the build phase."},{"location":"concepts/03-building/#second-phase-produces-the-final-container-image","text":"It just copies the binaries from the build container. FROM registry.access.redhat.com/ubi8/ubi-minimal:latest COPY --from=build /build/api /api USER 1001 CMD [\"/api\"] We are done :)","title":"Second phase produces the final container image"},{"location":"concepts/04-routing/","text":"HTTP Routing Here we are diving into the implementation of the service itself. For routing this service template uses go-chi library, as it is the most lightweight, but still does all we need. You can of course choose a different one if you like and the concepts will be very similar. We will place all relevant routing code under /internal/routes . Here we will not talk about metrics router, that is covered in Metrics Concept. Root router The entry point to the routing is routes.RootRouter() this sets up the root path router. This is / path on our server. This service uses this path for Liveness and Readiness probes of k8s. We will get to this later. For now the important part is that within this router we mount another API router onto a prefixed path. API router Our service will coexist with other services behind a shared public gateway. The gateway routes traffic by route prefix, but does not strip the matched prefix. So if our service will match for traffic on /api/template prefix, the full path gets forwarded. This means all our public facing paths need to be exposed with this prefix. The prefix is determined by routes.PathPrefix function. Individual routes All routes have their matching pattern and a http.HandlerFunc . Handler function is a function that receives a request as a parameter and writes response in a specialized IO. Our use of handlers is basically Controller from an MVC apps. We will put all our handlers in a /internal/services folder, more on that later. Chi routes are defined by calling function named by their matching HTTP verb. For example router.Get(\"/hello\", Handler) creates a GET route for pattern /hello . Grouping routes We can group routes under one prefix and only define the suffix for them. To define the route with a prefix we would do following. router.Route(\"/prefix\", func(subRouter chi.Router) { subRouter.Get(\"/hello\", Handler) }) HTTP server To put this all together, we will put following code in our api entry point and start a http server. // cmd/api/main.go rootRouter := routes.RootRouter() apiServer := http.Server{ Addr: fmt.Sprintf(\":%d\", 8000), Handler: rootRouter, } Later we will make this port configurable. Start listening Following code starts up the server we've set up. It will write out error message unless the server has been stopped gracefully. if err := apiServer.ListenAndServe(); err != nil { if !errors.Is(err, http.ErrServerClosed) { log.Fatal().Err(err).Msg(\"Main service listen error\") } } Stop listening Here we will cover graceful shutdown of our server. We won't cover all the details of following code. We set up a channel that will get notified when we receive SIGINT or SIGTERM . The signal wait is blocking, so we are waiting for it in separate goroutine. We have another channel that we wait for in our main and once this gets triggered, we finish.","title":"Routing"},{"location":"concepts/04-routing/#http-routing","text":"Here we are diving into the implementation of the service itself. For routing this service template uses go-chi library, as it is the most lightweight, but still does all we need. You can of course choose a different one if you like and the concepts will be very similar. We will place all relevant routing code under /internal/routes . Here we will not talk about metrics router, that is covered in Metrics Concept.","title":"HTTP Routing"},{"location":"concepts/04-routing/#root-router","text":"The entry point to the routing is routes.RootRouter() this sets up the root path router. This is / path on our server. This service uses this path for Liveness and Readiness probes of k8s. We will get to this later. For now the important part is that within this router we mount another API router onto a prefixed path.","title":"Root router"},{"location":"concepts/04-routing/#api-router","text":"Our service will coexist with other services behind a shared public gateway. The gateway routes traffic by route prefix, but does not strip the matched prefix. So if our service will match for traffic on /api/template prefix, the full path gets forwarded. This means all our public facing paths need to be exposed with this prefix. The prefix is determined by routes.PathPrefix function.","title":"API router"},{"location":"concepts/04-routing/#individual-routes","text":"All routes have their matching pattern and a http.HandlerFunc . Handler function is a function that receives a request as a parameter and writes response in a specialized IO. Our use of handlers is basically Controller from an MVC apps. We will put all our handlers in a /internal/services folder, more on that later. Chi routes are defined by calling function named by their matching HTTP verb. For example router.Get(\"/hello\", Handler) creates a GET route for pattern /hello .","title":"Individual routes"},{"location":"concepts/04-routing/#grouping-routes","text":"We can group routes under one prefix and only define the suffix for them. To define the route with a prefix we would do following. router.Route(\"/prefix\", func(subRouter chi.Router) { subRouter.Get(\"/hello\", Handler) })","title":"Grouping routes"},{"location":"concepts/04-routing/#http-server","text":"To put this all together, we will put following code in our api entry point and start a http server. // cmd/api/main.go rootRouter := routes.RootRouter() apiServer := http.Server{ Addr: fmt.Sprintf(\":%d\", 8000), Handler: rootRouter, } Later we will make this port configurable.","title":"HTTP server"},{"location":"concepts/04-routing/#start-listening","text":"Following code starts up the server we've set up. It will write out error message unless the server has been stopped gracefully. if err := apiServer.ListenAndServe(); err != nil { if !errors.Is(err, http.ErrServerClosed) { log.Fatal().Err(err).Msg(\"Main service listen error\") } }","title":"Start listening"},{"location":"concepts/04-routing/#stop-listening","text":"Here we will cover graceful shutdown of our server. We won't cover all the details of following code. We set up a channel that will get notified when we receive SIGINT or SIGTERM . The signal wait is blocking, so we are waiting for it in separate goroutine. We have another channel that we wait for in our main and once this gets triggered, we finish.","title":"Stop listening"},{"location":"concepts/05-logging/","text":"Logging This template introduces zerolog over platform recommended logrus . Until this is interchangeable Authors feel like zerolog is faster, easier to set up and use and has smaller memory footprint. The global logger is set up at the start of our service. For every request we will set up a new context logger and store it into a request context. We use a logging middleware to do this. This approach makes it easy to add additional fields and these are passed to all log entries. Context should be kept very small as it gets passed very often through the whole stack. Zerolog has such a small footprint that this is ok to do. This approach standardize logging into a pattern of fetching logger from context and logging with help of this logger. Every function can then be called in any context. We are always sure it logs correct log identifiers. Log output - cloudwatch We need to set up logging output. The template has two outputs. Stdout writer used for development and for CI pipelines is very easy to initialize. stdWriter := zerolog.ConsoleWriter{ Out: os.Stdout, TimeFormat: time.Kitchen, } Second for production logging. In production, we are logging to Amazon Cloudwatch. From Cloudwatch, the platform team automatically pulls the logs to our Kibana. The service responsibility is only to deliver logs to Cloudwatch. To set up Cloudwatch writer, in our logging.InitializeLogger function, we need Cloudwatch credentials, we will get to that in next chapter. Following snippet initializes Cloudwatch writer assuming the credentials variables. func newCloudwatchWriter(region string, key string, secret string, session string, logGroup string, logStream string) (*io.Writer, error) { cache := aws.NewCredentialsCache(credentials.NewStaticCredentialsProvider(key, secret, session)) cwClient := cloudwatchlogs.New(cloudwatchlogs.Options{ Region: region, Credentials: cache, }) cloudWatchWriter, err := cloudwatchwriter2.NewWithClient(cwClient, 500*time.Millisecond, logGroup, logStream) return cloudWatchWriter, err } Now when we will be ready to decide whether we want to use Cloudwatch or Stdout, we are ready to use the following to initialize our logger. zerolog.SetGlobalLevel(level) //nolint:reassign zerolog.ErrorStackMarshaler = pkgerrors.MarshalStack output := initializeLogOutput() // here we will need to decide on the output used. logger := zerolog.New(output) // decorate logger (and thus every log line) with hostname and timestamp logger = logger.With().Timestamp().Str(\"hostname\", hostname).Logger() Log middleware Middleware in Golang is a function that takes next http.Handler as parameter and returns http.Handler itself. The inner http.Handler needs to call next.ServeHTTP() to invoke the following handler. The simplest middleware is a function, that is called on ServeHTTP() , go provides a helper http.HandlerFunc to create such a function We further wrap that with a higher order function that allows us to pass logger. In case we want to change logger we want to use, it would be easier to do then if the middleware would use the global logger directly. The middleware enhances the global (passed in) logger with the request context fields like remote IP, request path, HTTP method. Then we log the very first line of every request. loggerCtx := globalLogger.With(). Str(\"remote_ip\", r.RemoteAddr). Str(\"url\", r.URL.Path). Str(\"method\", r.Method) contextLogger := loggerCtx.Logger() contextLogger.Debug().Msgf(\"Started %s request %s\", r.Method, r.URL.Path) The major thing we do here is to store our logger into the context that we pass down the middleware stack. // see logging.WithLogger() ctx := WithLogger(r.Context(), &contextLogger) next.ServeHTTP(ww, r.WithContext(ctx)) We follow up with deferring a function to log last log line of every request. We run it deferred, effectively after all following middlewares and thus at the end of the request. This function has one very special effect, it recovers from panic and logs it. Every panic that happens further down the middleware stack is thus recovered and logged in our middleware. t1 := time.Now() defer func() { duration := time.Since(t1) afterLogger := contextLogger.With(). Dur(\"latency_ms\", duration). Int(\"bytes_in\", bytesIn). Int(\"bytes_out\", ww.BytesWritten()). Logger() // prevent the application from exiting if rec := recover(); rec != nil { afterLogger.Error(). Bool(\"panic\", true). Int(\"status\", http.StatusInternalServerError). Msgf(\"Unhandled panic: %s\\n%s\", rec, debug.Stack()) http.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError) } log.Info(). Int(\"status\", ww.Status()). Msgf(\"Completed %s request %s in %s with %d\", r.Method, r.URL.Path, duration.Round(time.Millisecond).String(), ww.Status()) }() Add middleware to stack Now the last thing we need to do is to add our middleware to the API router. // internal/routes/api_router.go router.Use(logging.NewMiddleware(log.Logger)) And we are all setup. Using the logger Now we have the logger in the context, we will be passing context through the app. To use the logger, we will do following. logger := logging.Logger(ctx) logger.Info().Msg(\"Message one.\") logger.Debug().Msg(\"Message two.\") Happy logging! :)","title":"Logging"},{"location":"concepts/05-logging/#logging","text":"This template introduces zerolog over platform recommended logrus . Until this is interchangeable Authors feel like zerolog is faster, easier to set up and use and has smaller memory footprint. The global logger is set up at the start of our service. For every request we will set up a new context logger and store it into a request context. We use a logging middleware to do this. This approach makes it easy to add additional fields and these are passed to all log entries. Context should be kept very small as it gets passed very often through the whole stack. Zerolog has such a small footprint that this is ok to do. This approach standardize logging into a pattern of fetching logger from context and logging with help of this logger. Every function can then be called in any context. We are always sure it logs correct log identifiers.","title":"Logging"},{"location":"concepts/05-logging/#log-output-cloudwatch","text":"We need to set up logging output. The template has two outputs. Stdout writer used for development and for CI pipelines is very easy to initialize. stdWriter := zerolog.ConsoleWriter{ Out: os.Stdout, TimeFormat: time.Kitchen, } Second for production logging. In production, we are logging to Amazon Cloudwatch. From Cloudwatch, the platform team automatically pulls the logs to our Kibana. The service responsibility is only to deliver logs to Cloudwatch. To set up Cloudwatch writer, in our logging.InitializeLogger function, we need Cloudwatch credentials, we will get to that in next chapter. Following snippet initializes Cloudwatch writer assuming the credentials variables. func newCloudwatchWriter(region string, key string, secret string, session string, logGroup string, logStream string) (*io.Writer, error) { cache := aws.NewCredentialsCache(credentials.NewStaticCredentialsProvider(key, secret, session)) cwClient := cloudwatchlogs.New(cloudwatchlogs.Options{ Region: region, Credentials: cache, }) cloudWatchWriter, err := cloudwatchwriter2.NewWithClient(cwClient, 500*time.Millisecond, logGroup, logStream) return cloudWatchWriter, err } Now when we will be ready to decide whether we want to use Cloudwatch or Stdout, we are ready to use the following to initialize our logger. zerolog.SetGlobalLevel(level) //nolint:reassign zerolog.ErrorStackMarshaler = pkgerrors.MarshalStack output := initializeLogOutput() // here we will need to decide on the output used. logger := zerolog.New(output) // decorate logger (and thus every log line) with hostname and timestamp logger = logger.With().Timestamp().Str(\"hostname\", hostname).Logger()","title":"Log output - cloudwatch"},{"location":"concepts/05-logging/#log-middleware","text":"Middleware in Golang is a function that takes next http.Handler as parameter and returns http.Handler itself. The inner http.Handler needs to call next.ServeHTTP() to invoke the following handler. The simplest middleware is a function, that is called on ServeHTTP() , go provides a helper http.HandlerFunc to create such a function We further wrap that with a higher order function that allows us to pass logger. In case we want to change logger we want to use, it would be easier to do then if the middleware would use the global logger directly. The middleware enhances the global (passed in) logger with the request context fields like remote IP, request path, HTTP method. Then we log the very first line of every request. loggerCtx := globalLogger.With(). Str(\"remote_ip\", r.RemoteAddr). Str(\"url\", r.URL.Path). Str(\"method\", r.Method) contextLogger := loggerCtx.Logger() contextLogger.Debug().Msgf(\"Started %s request %s\", r.Method, r.URL.Path) The major thing we do here is to store our logger into the context that we pass down the middleware stack. // see logging.WithLogger() ctx := WithLogger(r.Context(), &contextLogger) next.ServeHTTP(ww, r.WithContext(ctx)) We follow up with deferring a function to log last log line of every request. We run it deferred, effectively after all following middlewares and thus at the end of the request. This function has one very special effect, it recovers from panic and logs it. Every panic that happens further down the middleware stack is thus recovered and logged in our middleware. t1 := time.Now() defer func() { duration := time.Since(t1) afterLogger := contextLogger.With(). Dur(\"latency_ms\", duration). Int(\"bytes_in\", bytesIn). Int(\"bytes_out\", ww.BytesWritten()). Logger() // prevent the application from exiting if rec := recover(); rec != nil { afterLogger.Error(). Bool(\"panic\", true). Int(\"status\", http.StatusInternalServerError). Msgf(\"Unhandled panic: %s\\n%s\", rec, debug.Stack()) http.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError) } log.Info(). Int(\"status\", ww.Status()). Msgf(\"Completed %s request %s in %s with %d\", r.Method, r.URL.Path, duration.Round(time.Millisecond).String(), ww.Status()) }()","title":"Log middleware"},{"location":"concepts/05-logging/#add-middleware-to-stack","text":"Now the last thing we need to do is to add our middleware to the API router. // internal/routes/api_router.go router.Use(logging.NewMiddleware(log.Logger)) And we are all setup.","title":"Add middleware to stack"},{"location":"concepts/05-logging/#using-the-logger","text":"Now we have the logger in the context, we will be passing context through the app. To use the logger, we will do following. logger := logging.Logger(ctx) logger.Info().Msg(\"Message one.\") logger.Debug().Msg(\"Message two.\") Happy logging! :)","title":"Using the logger"},{"location":"concepts/06-configuration/","text":"Configuration and clowder To customize our app across environments we need some configuration. In addition to standard application configuration we need to consume configuration from Red Hat Insights deployer clowder . Basic configuration We define structure config in internal/config/config.go to hold all our configuration variables. We have nested structures grouping configuration variables by category. We export these categories by referencing them by exported variables on package level Category = &config.Category . Container application configuration is mostly based on environment variables. We want to have all our environments close. We load all configuration from environment variables. To configure our app locally for development and unit testing, we will use .env files that stay close production setup. To make this easy locally, we provide example configuration, that can be regenerated by make generate-example-config . To load configuration just run Initialize with the .env file as parameter. In the container this file does not exist and Initialize method will load config from environment. config.Initialize(\"config/api.env\") Clowder We will touch Clowder more during deployments as it is mainly a deployer operator. But as a deployer it wraps up many integrations to other services in production environments. Most notably for now it holds configurations of Database and Cloudwatch. Clowder adds these in a file in a container layer. There is a shared ConsoleDot library that loads this file for you and parses the config into go struct. Following snippet overrides Database and Cloudwatch config with Clowder provided config. import clowder \"github.com/redhatinsights/app-common-go/pkg/api/v1\" if clowder.IsClowderEnabled() { cfg := clowder.LoadedConfig // database config.Database.Host = cfg.Database.Hostname config.Database.Port = uint16(cfg.Database.Port) config.Database.User = cfg.Database.Username config.Database.Password = cfg.Database.Password config.Database.Name = cfg.Database.Name // cloudwatch (is blank in ephemeral) cw := cfg.Logging.Cloudwatch if cw.Region != \"\" && cw.AccessKeyId != \"\" && cw.SecretAccessKey != \"\" && cw.LogGroup != \"\" { config.Cloudwatch.Enabled = true config.Cloudwatch.Key = cw.AccessKeyId config.Cloudwatch.Secret = cw.SecretAccessKey config.Cloudwatch.Region = cw.Region config.Cloudwatch.Group = cw.LogGroup } } Use it :) Now the config package holds all your configuration at the fingertip. You can access it anywhere from your app by config.Category.value . Remember the hardcoded http port? Now we can use config.Application.Port to configure it! :)","title":"Configuration"},{"location":"concepts/06-configuration/#configuration-and-clowder","text":"To customize our app across environments we need some configuration. In addition to standard application configuration we need to consume configuration from Red Hat Insights deployer clowder .","title":"Configuration and clowder"},{"location":"concepts/06-configuration/#basic-configuration","text":"We define structure config in internal/config/config.go to hold all our configuration variables. We have nested structures grouping configuration variables by category. We export these categories by referencing them by exported variables on package level Category = &config.Category . Container application configuration is mostly based on environment variables. We want to have all our environments close. We load all configuration from environment variables. To configure our app locally for development and unit testing, we will use .env files that stay close production setup. To make this easy locally, we provide example configuration, that can be regenerated by make generate-example-config . To load configuration just run Initialize with the .env file as parameter. In the container this file does not exist and Initialize method will load config from environment. config.Initialize(\"config/api.env\")","title":"Basic configuration"},{"location":"concepts/06-configuration/#clowder","text":"We will touch Clowder more during deployments as it is mainly a deployer operator. But as a deployer it wraps up many integrations to other services in production environments. Most notably for now it holds configurations of Database and Cloudwatch. Clowder adds these in a file in a container layer. There is a shared ConsoleDot library that loads this file for you and parses the config into go struct. Following snippet overrides Database and Cloudwatch config with Clowder provided config. import clowder \"github.com/redhatinsights/app-common-go/pkg/api/v1\" if clowder.IsClowderEnabled() { cfg := clowder.LoadedConfig // database config.Database.Host = cfg.Database.Hostname config.Database.Port = uint16(cfg.Database.Port) config.Database.User = cfg.Database.Username config.Database.Password = cfg.Database.Password config.Database.Name = cfg.Database.Name // cloudwatch (is blank in ephemeral) cw := cfg.Logging.Cloudwatch if cw.Region != \"\" && cw.AccessKeyId != \"\" && cw.SecretAccessKey != \"\" && cw.LogGroup != \"\" { config.Cloudwatch.Enabled = true config.Cloudwatch.Key = cw.AccessKeyId config.Cloudwatch.Secret = cw.SecretAccessKey config.Cloudwatch.Region = cw.Region config.Cloudwatch.Group = cw.LogGroup } }","title":"Clowder"},{"location":"concepts/06-configuration/#use-it","text":"Now the config package holds all your configuration at the fingertip. You can access it anywhere from your app by config.Category.value . Remember the hardcoded http port? Now we can use config.Application.Port to configure it! :)","title":"Use it :)"},{"location":"concepts/07-database/","text":"Database When we want to keep state, we most likely want a relational database for our service. ConsoleDot platform has settled on using PostgreSQL for all apps. This give apps clear choice for drivers, we can build our service with only single database in mind. When building a go application we could use GORM that deals with any database and has many ORM features. In this service tho, we expect to the database model being quite simple and use low level drivers. This gives us much more power to use pure go objects and strong typing. Migrations We use simple tool tern for migrations and migrations written in pure SQL. It allows to lock database during migrations. We use go embedding for migrations, so migrations are embedded into the migration binary. We are adding another binary for migrations and also a make target make migrate to run this binary. Code structure DB package We keep database initialization in a separate package. Here we just initialize the database connection global pool that can be accessed from other packages. Initialization db.Initialize accepts schema, we will use this for integration testing later on. DAO To organize our code, we want some structure and this service uses Data access objects. These are objects that abstract the data fetching process and define data access interfaces. This allows for easy database abstraction. We will use this during unit testing, to stub away database. Model Model is simple data structure that represents an entity of state. DAO methods Example dao method, that accepts a model and saves it in a database. // internal/dao/pgx/hello.go func (x *helloDaoPgx) Record(ctx context.Context, hello *models.Hello) error { query := ` INSERT INTO hellos (from, to, message) VALUES ($1, $2, $3) RETURNING id` err := db.Pool.QueryRow(ctx, query, hello.From, hello.To, hello.Message).Scan(&hello.ID) if err != nil { return fmt.Errorf(\"pgx error: %w\", err) } return nil } In the same file you can find also a List method for further examples. DAO initialization In the dao package, we have only the interfaces of the DAO implementations. For each DAO we have a getter, that allows us to initialize the required DAO. Our GetHello function returns the implementation of the Hello storage. In the implementation of this dao, we use the init trick to assign the implementation getter. // internal/dao/pgx/hello.go func init() { dao.GetHelloDao = getHelloDao } type helloDaoPgx struct{} func getHelloDao(ctx context.Context) dao.HelloDao { return &helloDaoPgx{} } Using DAO from services In our service handlers, we just want to initialize the implementation. These files should not care what the implementation is and only cares about it implementing the interface. helloDao := dao.GetHelloDao(r.Context()) hellos, err := helloDao.List(r.Context(), 100, 0) For simplicity, we are using a static limit 100 and offset 0 . Where do we initialize the implementation? In the main of our API! We are choosing the implementation per binary, so we can switch implementation for tests. It's a bit inconvenient as it can easily be forgotten, but the panic caused by it, is easily recognizable.","title":"Database"},{"location":"concepts/07-database/#database","text":"When we want to keep state, we most likely want a relational database for our service. ConsoleDot platform has settled on using PostgreSQL for all apps. This give apps clear choice for drivers, we can build our service with only single database in mind. When building a go application we could use GORM that deals with any database and has many ORM features. In this service tho, we expect to the database model being quite simple and use low level drivers. This gives us much more power to use pure go objects and strong typing.","title":"Database"},{"location":"concepts/07-database/#migrations","text":"We use simple tool tern for migrations and migrations written in pure SQL. It allows to lock database during migrations. We use go embedding for migrations, so migrations are embedded into the migration binary. We are adding another binary for migrations and also a make target make migrate to run this binary.","title":"Migrations"},{"location":"concepts/07-database/#code-structure","text":"","title":"Code structure"},{"location":"concepts/07-database/#db-package","text":"We keep database initialization in a separate package. Here we just initialize the database connection global pool that can be accessed from other packages. Initialization db.Initialize accepts schema, we will use this for integration testing later on.","title":"DB package"},{"location":"concepts/07-database/#dao","text":"To organize our code, we want some structure and this service uses Data access objects. These are objects that abstract the data fetching process and define data access interfaces. This allows for easy database abstraction. We will use this during unit testing, to stub away database.","title":"DAO"},{"location":"concepts/07-database/#model","text":"Model is simple data structure that represents an entity of state.","title":"Model"},{"location":"concepts/07-database/#dao-methods","text":"Example dao method, that accepts a model and saves it in a database. // internal/dao/pgx/hello.go func (x *helloDaoPgx) Record(ctx context.Context, hello *models.Hello) error { query := ` INSERT INTO hellos (from, to, message) VALUES ($1, $2, $3) RETURNING id` err := db.Pool.QueryRow(ctx, query, hello.From, hello.To, hello.Message).Scan(&hello.ID) if err != nil { return fmt.Errorf(\"pgx error: %w\", err) } return nil } In the same file you can find also a List method for further examples.","title":"DAO methods"},{"location":"concepts/07-database/#dao-initialization","text":"In the dao package, we have only the interfaces of the DAO implementations. For each DAO we have a getter, that allows us to initialize the required DAO. Our GetHello function returns the implementation of the Hello storage. In the implementation of this dao, we use the init trick to assign the implementation getter. // internal/dao/pgx/hello.go func init() { dao.GetHelloDao = getHelloDao } type helloDaoPgx struct{} func getHelloDao(ctx context.Context) dao.HelloDao { return &helloDaoPgx{} }","title":"DAO initialization"},{"location":"concepts/07-database/#using-dao-from-services","text":"In our service handlers, we just want to initialize the implementation. These files should not care what the implementation is and only cares about it implementing the interface. helloDao := dao.GetHelloDao(r.Context()) hellos, err := helloDao.List(r.Context(), 100, 0) For simplicity, we are using a static limit 100 and offset 0 . Where do we initialize the implementation? In the main of our API! We are choosing the implementation per binary, so we can switch implementation for tests. It's a bit inconvenient as it can easily be forgotten, but the panic caused by it, is easily recognizable.","title":"Using DAO from services"},{"location":"concepts/08-openapi/","text":"OpenAPI requests and responses Here we cover how to maintain OpenAPI spec and how to keep the API in accordance to the spec. Serving OpenAPI spec is a hard requirement for every service on ConsoleDot. It serves as an API contract, for other services to know what to expect from your service. This spec should be always backward compatible starting your first release. If you need to make incompatible changes, you will need to keep the old spec, and it's corresponding API working to allow for migrations. Please keep that in mind while designing the spec. It's convenient to get it somewhat right on the first release. Note: first release not the first commit, we do not encourage over designing your API upfront, we know it will change as you implement it, but keep in mind to keep the OpenAPI as tidy as possible. :) Generate OpenAPI spec There are generally three major approaches to the OpenAPI spec to keep it in sync with your API. Generate API service from the spec. Generate OpenAPI spec from the services Hybrid approach There are advantages to all of these approach, and it's up to you to choose one. This service template covers hybrid approach as some services might leverage the control it gives to maintainers. Note: hybrid approach has more variants, feel free to explore if the choice makes you uncomfortable. Hybrid approach in our case means we will generate request and response types from Go to OpenAPI. The rest - paths, various response codes are kept manually. This gives the highest level of freedom while still taking care of the most of the manual burden behind keeping the types in sync from Go to OpenAPI. Payloads To maintain our request and response types, we will use payload types that implement Chi's render interface, see go-chi/render for more details. We will store our payloads in separate folder, these will use models to generate responses. We can easily use type embedding for the payloads and just copy the model fields, but it is discouraged. You might accidentally copy and expose fields you don't want. We will be copying the data instead. We want our users to be able to define a sender of a message and a message, but we will define recipient statically, and not allow setting it through the create request. We are also renaming the internal columns to expose them under different names in the API to showcase what you might leverage the payloads for. Payload is a go struct that has json tag for bindings from and to JSON. Following is a basic example where request and response have the same data. There is more advance example with comments in hello_payload.go . // internal/payloads/hello_payload.go type HelloPayload struct { ID uint64 `json:\"id\"` Sender string `json:\"sender\"` Message string `json:\"message\"` } type ( HelloRequest HelloPayload HelloResponse HelloPayload ) func (req HelloRequest) Bind(_ *http.Request) error { return nil } func (req HelloResponse) Render(_ http.ResponseWriter, _ *http.Request) error { return nil } Using payloads We use payloads in the http handlers to bind and render data to and from JSON. First lets take a look at binding data from json request to Go struct. // internal/services/hello_service.go import \"github.com/go-chi/render\" func SayHello(w http.ResponseWriter, r *http.Request) { payload := payloads.HelloRequest{} if err := render.Bind(r, payload); err != nil { // Error handling TBD return } } And now let see rendering a JSON response. func ListHellos(w http.ResponseWriter, r *http.Request) { helloDao := dao.GetHelloDao(r.Context()) hellos, err := helloDao.List(r.Context(), 100, 0) // error handling TBD if renderErr := render.RenderList(w, r, payloads.NewHelloListResponse(hellos)); renderErr != nil { // error handling TBD } } Error handling In above examples, we have left out the error handling. Let us dive into it here. Errors are just another type of payload we want to emit to user. The main difference here is, that we want to standardize the payload structure across all handlers. We will have a helper render function to render the error payloads. As rendering the payload itself can error out, we want to have a fallback to general 500 error rendering in this helper. // internal/services/error_renderer.go func renderError(w http.ResponseWriter, r *http.Request, renderer render.Renderer) { if renderErr := render.Render(w, r, renderer); renderErr != nil { writeBasicError(w, r, renderErr) // this is a fallback } } Our helper expects a chi renderer, so we need a payload to pass in. For this purpose we introduce error payloads in internal/payloads/error_payload.go . The following is a basis for all our error payloads. // internal/payloads/error_payload.go // ResponseError is used as a payload for all errors type ErrorResponse struct { // HTTP status code HTTPStatusCode int `json:\"-\"` // user facing error message Message string `json:\"msg\"` // full root cause Error string `json:\"error\"` } func (e ErrorResponse) Render(_ http.ResponseWriter, r *http.Request) error { render.Status(r, e.HTTPStatusCode) return nil } func newErrorResponse(ctx context.Context, status int, userMsg string, err error) ErrorResponse { return &ErrorResponse{ HTTPStatusCode: status, Message: userMsg, Error: err.Error(), } } In previous code snippets, we have seen three types of errors. Bad request error, which should be status 400. Database error, which can be not found and should have status 404, or all other DB errors status 500. General code error and redner error, which are both 500 status. Let's add payload constructors for these errors. Notice that all of these have user message. The go error holds very useful debugging info and can be quite helpful to understand what went technically wrong. Usually we want to convey the main message to users and make it more user readable. That's where the user message comes in. func NewInvalidRequestError(ctx context.Context, message string, err error) *ResponseError { message = fmt.Sprintf(\"Invalid request: %s\", message) return newResponse(ctx, http.StatusBadRequest, message, err) } func NewDAOError(ctx context.Context, message string, err error) *ResponseError { message = fmt.Sprintf(\"DAO error: %s\", message) return newResponse(ctx, http.StatusInternalServerError, message, err) } func NewRenderError(ctx context.Context, message string, err error) *ResponseError { message = fmt.Sprintf(\"Rendering error: %s\", message) return newResponse(ctx, http.StatusInternalServerError, message, err) } We want to keep these simple, so for the dao error, we will introduce one more render helper. Here we will wrap the decision whether to render NotFound or Internal error. // internal/services/error_renderer.go func renderNotFoundOrDAOError(w http.ResponseWriter, r *http.Request, err error, resource string) { if errors.Is(err, dao.ErrNoRows) { renderError(w, r, payloads.NewNotFoundError(r.Context(), resource, err)) } else { renderError(w, r, payloads.NewDAOError(r.Context(), resource, err)) } } Take a look at the code for the full implementation. OpenAPI generator We will add another binary openapi_spec for generating our OpenAPI spec. We will use a kin-openapi 's generator to do the heavy lifting for us. We won't go in details of the generating itself. There are two notable aspect we need to pay attention to. We have a file cmd/openapi_spec/paths.yml alongside the binary to manually maintain routes and used request and response schemas. In the binary main.go itself we need to list all the payload schemas in method addPayloads . It is kept as first function for convenience. The spec will be generated to both JSON and YAML. It is straight forward to get rid one of the formats if you don't find it useful. At this point it should be straight forward to add more types or routes to our spec. OpenAPI spec endpoint The app should serve it's current spec for easier integration with some apps. We will use embedding in the /api package and create a http handler, that will write the generated json spec. This is all we need: // api/openapi_handler.go func ServeOpenAPISpec(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") if _, err := w.Write(embeddedJSONSpec); err != nil { w.WriteHeader(http.StatusInternalServerError) _, _ = w.Write([]byte(fmt.Sprintf(`{\"msg\": \"%s\"`, err.Error()))) } } To return this spec on a path /api/prefix/v1/openapi.json we will create a new route in the api router. router.Get(\"/openapi.json\", api.ServeOpenAPISpec)","title":"OpenAPI"},{"location":"concepts/08-openapi/#openapi-requests-and-responses","text":"Here we cover how to maintain OpenAPI spec and how to keep the API in accordance to the spec. Serving OpenAPI spec is a hard requirement for every service on ConsoleDot. It serves as an API contract, for other services to know what to expect from your service. This spec should be always backward compatible starting your first release. If you need to make incompatible changes, you will need to keep the old spec, and it's corresponding API working to allow for migrations. Please keep that in mind while designing the spec. It's convenient to get it somewhat right on the first release. Note: first release not the first commit, we do not encourage over designing your API upfront, we know it will change as you implement it, but keep in mind to keep the OpenAPI as tidy as possible. :)","title":"OpenAPI requests and responses"},{"location":"concepts/08-openapi/#generate-openapi-spec","text":"There are generally three major approaches to the OpenAPI spec to keep it in sync with your API. Generate API service from the spec. Generate OpenAPI spec from the services Hybrid approach There are advantages to all of these approach, and it's up to you to choose one. This service template covers hybrid approach as some services might leverage the control it gives to maintainers. Note: hybrid approach has more variants, feel free to explore if the choice makes you uncomfortable. Hybrid approach in our case means we will generate request and response types from Go to OpenAPI. The rest - paths, various response codes are kept manually. This gives the highest level of freedom while still taking care of the most of the manual burden behind keeping the types in sync from Go to OpenAPI.","title":"Generate OpenAPI spec"},{"location":"concepts/08-openapi/#payloads","text":"To maintain our request and response types, we will use payload types that implement Chi's render interface, see go-chi/render for more details. We will store our payloads in separate folder, these will use models to generate responses. We can easily use type embedding for the payloads and just copy the model fields, but it is discouraged. You might accidentally copy and expose fields you don't want. We will be copying the data instead. We want our users to be able to define a sender of a message and a message, but we will define recipient statically, and not allow setting it through the create request. We are also renaming the internal columns to expose them under different names in the API to showcase what you might leverage the payloads for. Payload is a go struct that has json tag for bindings from and to JSON. Following is a basic example where request and response have the same data. There is more advance example with comments in hello_payload.go . // internal/payloads/hello_payload.go type HelloPayload struct { ID uint64 `json:\"id\"` Sender string `json:\"sender\"` Message string `json:\"message\"` } type ( HelloRequest HelloPayload HelloResponse HelloPayload ) func (req HelloRequest) Bind(_ *http.Request) error { return nil } func (req HelloResponse) Render(_ http.ResponseWriter, _ *http.Request) error { return nil }","title":"Payloads"},{"location":"concepts/08-openapi/#using-payloads","text":"We use payloads in the http handlers to bind and render data to and from JSON. First lets take a look at binding data from json request to Go struct. // internal/services/hello_service.go import \"github.com/go-chi/render\" func SayHello(w http.ResponseWriter, r *http.Request) { payload := payloads.HelloRequest{} if err := render.Bind(r, payload); err != nil { // Error handling TBD return } } And now let see rendering a JSON response. func ListHellos(w http.ResponseWriter, r *http.Request) { helloDao := dao.GetHelloDao(r.Context()) hellos, err := helloDao.List(r.Context(), 100, 0) // error handling TBD if renderErr := render.RenderList(w, r, payloads.NewHelloListResponse(hellos)); renderErr != nil { // error handling TBD } }","title":"Using payloads"},{"location":"concepts/08-openapi/#error-handling","text":"In above examples, we have left out the error handling. Let us dive into it here. Errors are just another type of payload we want to emit to user. The main difference here is, that we want to standardize the payload structure across all handlers. We will have a helper render function to render the error payloads. As rendering the payload itself can error out, we want to have a fallback to general 500 error rendering in this helper. // internal/services/error_renderer.go func renderError(w http.ResponseWriter, r *http.Request, renderer render.Renderer) { if renderErr := render.Render(w, r, renderer); renderErr != nil { writeBasicError(w, r, renderErr) // this is a fallback } } Our helper expects a chi renderer, so we need a payload to pass in. For this purpose we introduce error payloads in internal/payloads/error_payload.go . The following is a basis for all our error payloads. // internal/payloads/error_payload.go // ResponseError is used as a payload for all errors type ErrorResponse struct { // HTTP status code HTTPStatusCode int `json:\"-\"` // user facing error message Message string `json:\"msg\"` // full root cause Error string `json:\"error\"` } func (e ErrorResponse) Render(_ http.ResponseWriter, r *http.Request) error { render.Status(r, e.HTTPStatusCode) return nil } func newErrorResponse(ctx context.Context, status int, userMsg string, err error) ErrorResponse { return &ErrorResponse{ HTTPStatusCode: status, Message: userMsg, Error: err.Error(), } } In previous code snippets, we have seen three types of errors. Bad request error, which should be status 400. Database error, which can be not found and should have status 404, or all other DB errors status 500. General code error and redner error, which are both 500 status. Let's add payload constructors for these errors. Notice that all of these have user message. The go error holds very useful debugging info and can be quite helpful to understand what went technically wrong. Usually we want to convey the main message to users and make it more user readable. That's where the user message comes in. func NewInvalidRequestError(ctx context.Context, message string, err error) *ResponseError { message = fmt.Sprintf(\"Invalid request: %s\", message) return newResponse(ctx, http.StatusBadRequest, message, err) } func NewDAOError(ctx context.Context, message string, err error) *ResponseError { message = fmt.Sprintf(\"DAO error: %s\", message) return newResponse(ctx, http.StatusInternalServerError, message, err) } func NewRenderError(ctx context.Context, message string, err error) *ResponseError { message = fmt.Sprintf(\"Rendering error: %s\", message) return newResponse(ctx, http.StatusInternalServerError, message, err) } We want to keep these simple, so for the dao error, we will introduce one more render helper. Here we will wrap the decision whether to render NotFound or Internal error. // internal/services/error_renderer.go func renderNotFoundOrDAOError(w http.ResponseWriter, r *http.Request, err error, resource string) { if errors.Is(err, dao.ErrNoRows) { renderError(w, r, payloads.NewNotFoundError(r.Context(), resource, err)) } else { renderError(w, r, payloads.NewDAOError(r.Context(), resource, err)) } } Take a look at the code for the full implementation.","title":"Error handling"},{"location":"concepts/08-openapi/#openapi-generator","text":"We will add another binary openapi_spec for generating our OpenAPI spec. We will use a kin-openapi 's generator to do the heavy lifting for us. We won't go in details of the generating itself. There are two notable aspect we need to pay attention to. We have a file cmd/openapi_spec/paths.yml alongside the binary to manually maintain routes and used request and response schemas. In the binary main.go itself we need to list all the payload schemas in method addPayloads . It is kept as first function for convenience. The spec will be generated to both JSON and YAML. It is straight forward to get rid one of the formats if you don't find it useful. At this point it should be straight forward to add more types or routes to our spec.","title":"OpenAPI generator"},{"location":"concepts/08-openapi/#openapi-spec-endpoint","text":"The app should serve it's current spec for easier integration with some apps. We will use embedding in the /api package and create a http handler, that will write the generated json spec. This is all we need: // api/openapi_handler.go func ServeOpenAPISpec(w http.ResponseWriter, r *http.Request) { w.Header().Set(\"Content-Type\", \"application/json\") if _, err := w.Write(embeddedJSONSpec); err != nil { w.WriteHeader(http.StatusInternalServerError) _, _ = w.Write([]byte(fmt.Sprintf(`{\"msg\": \"%s\"`, err.Error()))) } } To return this spec on a path /api/prefix/v1/openapi.json we will create a new route in the api router. router.Get(\"/openapi.json\", api.ServeOpenAPISpec)","title":"OpenAPI spec endpoint"},{"location":"concepts/09-testing/","text":"Testing Here we cover how to test our Go code, we are only covering unit testing of the code. Integration and system testing of ConsoleDot application should be done by a IQE plugin We have an application that has documented API and on request writes data to a database. Testing strategy Testing strategy chosen for this template is: Tests are as isolated as possible isolate between the tests isolate the testing layer from other application layers Test are testing the public API of a package avoid using internal functions public interface means interface of given layer, not a user interface (API) We have two suites of tests. One is classical Go unit tests, second is for database tests. Stubbing Now we can dive deeper on the idea of multiple implementation for a code layer. We have an interface for our DAO, which gives us an option to add another implementation. This other implementation will be storing the data in memory. Stubbed layer makes tests significantly faster and simplifies the DAO code to a bare minimum. We also keep the stub in a context. Thanks to context isolation, the tests are runnable in parallel without leaking data. The minimal stub reservation looks like this. We are skipping implementation of the context setter and getter here. func init() { dao.GetHelloDao = getHelloDao } type helloDaoStub struct { store []*models.Hello } func getHelloDao(ctx context.Context) dao.HelloDao { return getHelloDaoStub(ctx) } func (x *helloDaoStub) List(ctx context.Context, limit, offset int64) ([]*models.Hello, error) { return x.store, nil } func (x *helloDaoStub) Record(ctx context.Context, hello *models.Hello) error { hello.ID = int64(len(x.store)) x.store = append(x.store, hello) return nil } Now we are ready to write a handler test isolated from the underlying database. Handler tests Golang has a many features that make testing easier. We will cover the most basic, but there is much more! Ask in the community, not all the features are as well documented as go production code. Let see the simplest test we can have to get into testing. The following example shows how to set up a context with: * the prepared DAO stub, * stubbed request using directly http package testing helper, * response mock also using http package helper We are testing whether the empty response is rendered correctly. package services_test func TestListHellos(t *testing.T) { t.Run(\"handles empty database well\", func(t *testing.T) { ctx := stub.WithHelloDao(context.Background()) req, err := http.NewRequestWithContext(ctx, \"GET\", \"/api/template/hellos\", nil) require.NoError(t, err, \"failed to create request\") rr := httptest.NewRecorder() handler := http.HandlerFunc(services.ListHellos) handler.ServeHTTP(rr, req) require.Equal(t, http.StatusOK, rr.Code, \"Wrong status code\") assert.Equal(t, \"[]\\n\", rr.Body.String()) }) } We have already made few design decisions: * the test lives in a separate package * go recognizes a suffix _test as special package and allows two packages in the same folder * this makes it impossible to test internal unexported functions * We use a TestXX function to test function XX * We are nesting test cases using t.Run * We are using stretchr/testify to add a bit of syntactical sugger. Unit test suite Now we can add a make target make test that runs our tests. Database test suite We are stubbing database to speed up and isolate our unit tests. This is great, but how do we test our DAO tests. There are ways to test with in-memory databases, tho here we have decided to use real database. Database is a major integration, and it is not so slow we can't run these tests with ease, when limited to testing DAO methods. We have a testing main in internal/dao/tests/main.go and environment setup and teardown code in internal/dao/tests/environments.go . All the other files are testing files and best practice is to have a file per DAO. We aim at full coverage, to make sure our SQL queries are correct. As every test, if you are doing anything special in your DAO method, be sure to test for it. Database test suite can be run make test-database and we can set it up in a separate suite in CI.","title":"Testing"},{"location":"concepts/09-testing/#testing","text":"Here we cover how to test our Go code, we are only covering unit testing of the code. Integration and system testing of ConsoleDot application should be done by a IQE plugin We have an application that has documented API and on request writes data to a database.","title":"Testing"},{"location":"concepts/09-testing/#testing-strategy","text":"Testing strategy chosen for this template is: Tests are as isolated as possible isolate between the tests isolate the testing layer from other application layers Test are testing the public API of a package avoid using internal functions public interface means interface of given layer, not a user interface (API) We have two suites of tests. One is classical Go unit tests, second is for database tests.","title":"Testing strategy"},{"location":"concepts/09-testing/#stubbing","text":"Now we can dive deeper on the idea of multiple implementation for a code layer. We have an interface for our DAO, which gives us an option to add another implementation. This other implementation will be storing the data in memory. Stubbed layer makes tests significantly faster and simplifies the DAO code to a bare minimum. We also keep the stub in a context. Thanks to context isolation, the tests are runnable in parallel without leaking data. The minimal stub reservation looks like this. We are skipping implementation of the context setter and getter here. func init() { dao.GetHelloDao = getHelloDao } type helloDaoStub struct { store []*models.Hello } func getHelloDao(ctx context.Context) dao.HelloDao { return getHelloDaoStub(ctx) } func (x *helloDaoStub) List(ctx context.Context, limit, offset int64) ([]*models.Hello, error) { return x.store, nil } func (x *helloDaoStub) Record(ctx context.Context, hello *models.Hello) error { hello.ID = int64(len(x.store)) x.store = append(x.store, hello) return nil } Now we are ready to write a handler test isolated from the underlying database.","title":"Stubbing"},{"location":"concepts/09-testing/#handler-tests","text":"Golang has a many features that make testing easier. We will cover the most basic, but there is much more! Ask in the community, not all the features are as well documented as go production code. Let see the simplest test we can have to get into testing. The following example shows how to set up a context with: * the prepared DAO stub, * stubbed request using directly http package testing helper, * response mock also using http package helper We are testing whether the empty response is rendered correctly. package services_test func TestListHellos(t *testing.T) { t.Run(\"handles empty database well\", func(t *testing.T) { ctx := stub.WithHelloDao(context.Background()) req, err := http.NewRequestWithContext(ctx, \"GET\", \"/api/template/hellos\", nil) require.NoError(t, err, \"failed to create request\") rr := httptest.NewRecorder() handler := http.HandlerFunc(services.ListHellos) handler.ServeHTTP(rr, req) require.Equal(t, http.StatusOK, rr.Code, \"Wrong status code\") assert.Equal(t, \"[]\\n\", rr.Body.String()) }) } We have already made few design decisions: * the test lives in a separate package * go recognizes a suffix _test as special package and allows two packages in the same folder * this makes it impossible to test internal unexported functions * We use a TestXX function to test function XX * We are nesting test cases using t.Run * We are using stretchr/testify to add a bit of syntactical sugger.","title":"Handler tests"},{"location":"concepts/09-testing/#unit-test-suite","text":"Now we can add a make target make test that runs our tests.","title":"Unit test suite"},{"location":"concepts/09-testing/#database-test-suite","text":"We are stubbing database to speed up and isolate our unit tests. This is great, but how do we test our DAO tests. There are ways to test with in-memory databases, tho here we have decided to use real database. Database is a major integration, and it is not so slow we can't run these tests with ease, when limited to testing DAO methods. We have a testing main in internal/dao/tests/main.go and environment setup and teardown code in internal/dao/tests/environments.go . All the other files are testing files and best practice is to have a file per DAO. We aim at full coverage, to make sure our SQL queries are correct. As every test, if you are doing anything special in your DAO method, be sure to test for it. Database test suite can be run make test-database and we can set it up in a separate suite in CI.","title":"Database test suite"}]}